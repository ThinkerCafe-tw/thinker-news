"""
RSS Feed è®€å–æ¨¡çµ„
å¾å¤šå€‹ä¾†æºè®€å– RSS feedsï¼Œæ”¯æ´ timeoutã€retry èˆ‡å®¹éŒ¯æ©Ÿåˆ¶
"""

import feedparser
import logging
import time
import urllib.request
from datetime import datetime
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

# === RSS ä¾†æºé…ç½® ===
# region: 'intl' åœ‹éš› | 'tw' å°ç£ | 'ai' AI å°ˆé¡Œ
RSS_SOURCES = {
    # åœ‹éš›ç§‘æŠ€
    'hackernews':  {'url': 'https://feeds.feedburner.com/TheHackersNews',             'region': 'intl'},
    'techcrunch':  {'url': 'https://techcrunch.com/feed/',                             'region': 'intl'},
    'arstechnica': {'url': 'http://feeds.arstechnica.com/arstechnica/index/',          'region': 'intl'},
    # AI å°ˆé¡Œ
    'openai':      {'url': 'https://openai.com/news/rss.xml',                         'region': 'ai'},
    'bair':        {'url': 'https://bair.berkeley.edu/blog/feed.xml',                  'region': 'ai'},
    # å°ç£ç§‘æŠ€
    'technews':    {'url': 'https://technews.tw/feed/',                                'region': 'tw'},
    'ithome':      {'url': 'https://www.ithome.com.tw/rss',                            'region': 'tw'},
    'inside':      {'url': 'https://www.inside.com.tw/feed/rss',                       'region': 'tw'},
}

# é€£ç·šè¨­å®š
FETCH_TIMEOUT_SECS = 15   # å–®ä¸€ feed è®€å– timeout
MAX_RETRIES = 2            # å¤±æ•—é‡è©¦æ¬¡æ•¸
RETRY_DELAY_SECS = 2      # é‡è©¦é–“éš”


def fetch_single_feed(source_name: str, url: str) -> List[Dict]:
    """
    è®€å–å–®ä¸€ RSS feedï¼ˆå« timeout + retryï¼‰

    Args:
        source_name: ä¾†æºåç¨±
        url: RSS feed URL

    Returns:
        æ–°èåˆ—è¡¨ï¼ˆå¤±æ•—å›ç©º listï¼Œä¸æœƒ raiseï¼‰
    """
    last_error = None

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            logger.info(f"  ğŸ“¡ è®€å– {source_name}ï¼ˆattempt {attempt}ï¼‰...")

            # ç”¨ urllib æ‰‹å‹•æŠ“å†é¤µçµ¦ feedparserï¼Œæ‰èƒ½æ§åˆ¶ timeout
            req = urllib.request.Request(url, headers={'User-Agent': 'ThinkerNews/1.0'})
            with urllib.request.urlopen(req, timeout=FETCH_TIMEOUT_SECS) as resp:
                raw = resp.read()

            feed = feedparser.parse(raw)

            if feed.bozo and not feed.entries:
                logger.warning(f"  âš ï¸  {source_name} RSS æ ¼å¼æœ‰å•é¡Œä¸”ç„¡æ¢ç›®")
                last_error = f"bozo feed: {feed.bozo_exception}"
                time.sleep(RETRY_DELAY_SECS)
                continue

            news_items = []
            for entry in feed.entries:
                try:
                    item = {
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'content': entry.get('summary', entry.get('description', '')),
                        'pubDate': entry.get('published', entry.get('updated', '')),
                        'isoDate': entry.get('published_parsed', entry.get('updated_parsed', None)),
                        'source': source_name
                    }

                    # è½‰æ›æ—¥æœŸæ ¼å¼
                    if item['isoDate']:
                        try:
                            dt = datetime(*item['isoDate'][:6])
                            item['isoDate'] = dt.isoformat()
                        except Exception:
                            item['isoDate'] = ''

                    news_items.append(item)

                except Exception as e:
                    logger.warning(f"  âš ï¸  è™•ç† {source_name} çš„æŸå‰‡æ–°èæ™‚å‡ºéŒ¯: {e}")
                    continue

            logger.info(f"  âœ… {source_name}: è®€å– {len(news_items)} å‰‡")
            return news_items

        except Exception as e:
            last_error = str(e)
            logger.warning(f"  âš ï¸  {source_name} attempt {attempt} å¤±æ•—: {last_error}")
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY_SECS)

    logger.error(f"  âŒ {source_name} å…¨éƒ¨ {MAX_RETRIES} æ¬¡å˜—è©¦å¤±æ•—: {last_error}")
    return []


def fetch_all_rss_feeds(today_date: str) -> List[Dict]:
    """
    ä¸¦è¡Œè®€å–æ‰€æœ‰ RSS feeds

    Args:
        today_date: ä»Šæ—¥æ—¥æœŸï¼ˆç”¨æ–¼æ—¥èªŒï¼‰

    Returns:
        æ‰€æœ‰æ–°èçš„åˆ—è¡¨
    """
    all_news = []
    failed_sources = []

    max_workers = min(len(RSS_SOURCES), 8)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_source = {
            executor.submit(fetch_single_feed, name, cfg['url']): name
            for name, cfg in RSS_SOURCES.items()
        }

        for future in as_completed(future_to_source):
            source_name = future_to_source[future]
            try:
                news_items = future.result()
                if news_items:
                    all_news.extend(news_items)
                else:
                    failed_sources.append(source_name)
            except Exception as e:
                logger.error(f"âŒ {source_name} è®€å–ä»»å‹™å¤±æ•—: {e}")
                failed_sources.append(source_name)

    if failed_sources:
        logger.warning(f"âš ï¸  æœ¬æ¬¡å¤±æ•—ä¾†æº: {', '.join(failed_sources)}")

    logger.info(f"ğŸ“Š ç¸½å…±è®€å– {len(all_news)} å‰‡æ–°èï¼ˆä¾†è‡ª {len(RSS_SOURCES) - len(failed_sources)}/{len(RSS_SOURCES)} å€‹ä¾†æºï¼‰")
    return all_news
