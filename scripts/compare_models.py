#!/usr/bin/env python3
"""
æ¨¡åž‹å“è³ªæ¯”è¼ƒå¯¦é©—
ç”¨åŒä¸€æ‰¹æ–°èžæ¸¬è©¦ä¸åŒæ¨¡åž‹ï¼Œé ‚é…æ¨¡åž‹ç•¶è©•å¯©
"""
import json
import random
import os
from pathlib import Path
from datetime import datetime
from openai import OpenAI
import google.generativeai as genai

# ============================================
# æ¨¡åž‹é…ç½®
# ============================================

CP_MODELS = {
    "deepseek": {
        "provider": "deepseek",
        "model": "deepseek-chat",
    },
    "gemini-flash": {
        "provider": "google",
        "model": "gemini-2.0-flash",
    },
    "gemini-pro": {
        "provider": "google",
        "model": "gemini-1.5-pro",
    },
    "gpt4o-mini": {
        "provider": "openai",
        "model": "gpt-4o-mini",
    },
}

JUDGE_MODELS = {
    "gpt5": {
        "provider": "openai",
        "model": "gpt-5.3",  # æˆ–å¯¦éš›å¯ç”¨çš„æœ€æ–°ç‰ˆ
    },
    "claude-opus": {
        "provider": "anthropic",
        "model": "claude-opus-4-6",
    },
    "gemini-pro-top": {
        "provider": "google",
        "model": "gemini-2.5-pro",  # é ‚é…
    },
}

# ============================================
# API å®¢æˆ¶ç«¯
# ============================================

def get_openai_client():
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_deepseek_client():
    return OpenAI(
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com"
    )

def get_anthropic_client():
    from anthropic import Anthropic
    return Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

def init_google():
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# ============================================
# æ¨¡åž‹èª¿ç”¨
# ============================================

def call_model(model_config: dict, system: str, user: str) -> str:
    """çµ±ä¸€èª¿ç”¨ä¸åŒæ¨¡åž‹"""
    provider = model_config["provider"]
    model = model_config["model"]

    try:
        if provider == "openai":
            client = get_openai_client()
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user}
                ],
                temperature=0.7
            )
            return resp.choices[0].message.content

        elif provider == "deepseek":
            client = get_deepseek_client()
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user}
                ],
                temperature=0.7,
                max_tokens=4096
            )
            return resp.choices[0].message.content

        elif provider == "google":
            init_google()
            gmodel = genai.GenerativeModel(model)
            resp = gmodel.generate_content(f"{system}\n\n{user}")
            return resp.text

        elif provider == "anthropic":
            client = get_anthropic_client()
            resp = client.messages.create(
                model=model,
                max_tokens=4096,
                system=system,
                messages=[{"role": "user", "content": user}]
            )
            return resp.content[0].text

    except Exception as e:
        return f"[ERROR] {provider}/{model}: {e}"

# ============================================
# å¯¦é©—ä»»å‹™
# ============================================

TASK_PROMPT = """ä½ æ˜¯ AI ç§‘æŠ€æ–°èžç·¨è¼¯ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ–°èžè³‡æ–™ï¼Œæ’°å¯«ä¸€æ®µ 150-200 å­—çš„ç²¾è¯æ‘˜è¦ï¼ŒåŒ…å«ï¼š
1. æ–°èžé‡é»žï¼ˆä¸€å¥è©±ï¼‰
2. ç‚ºä»€éº¼é‡è¦ï¼ˆå°è®€è€…çš„æ„ç¾©ï¼‰
3. å­¸ç¿’å»ºè­°ï¼ˆçµ¦ AI åˆå­¸è€…çš„å•Ÿç™¼ï¼‰

æ–°èžè³‡æ–™ï¼š
æ¨™é¡Œï¼š{title}
ä¾†æºï¼š{source}
æ‘˜è¦ï¼š{summary}
é€£çµï¼š{link}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›žè¦†ï¼Œèªžæ°£å°ˆæ¥­ä½†å¹³æ˜“è¿‘äººã€‚"""

JUDGE_PROMPT = """ä½ æ˜¯è³‡æ·± AI å…§å®¹å“è³ªè©•å¯©ã€‚è«‹è©•ä¼°ä»¥ä¸‹ç”±ä¸åŒ AI æ¨¡åž‹ç”Ÿæˆçš„æ–°èžæ‘˜è¦ã€‚

åŽŸå§‹æ–°èžï¼š
æ¨™é¡Œï¼š{title}
æ‘˜è¦ï¼š{summary}

---

{outputs}

---

è«‹é‡å°æ¯å€‹è¼¸å‡ºè©•åˆ†ï¼ˆ1-10ï¼‰ï¼Œä¸¦èªªæ˜Žç†ç”±ã€‚è©•åˆ†æ¨™æº–ï¼š
- æº–ç¢ºæ€§ï¼ˆæ˜¯å¦æ­£ç¢ºç†è§£æ–°èžï¼‰
- å¯è®€æ€§ï¼ˆæ˜¯å¦æµæš¢æ˜“æ‡‚ï¼‰
- åƒ¹å€¼æ€§ï¼ˆæ˜¯å¦æä¾›æœ‰ç”¨è§€é»žï¼‰
- æ ¼å¼ï¼ˆæ˜¯å¦ç¬¦åˆè¦æ±‚çš„çµæ§‹ï¼‰

è«‹ç”¨ JSON æ ¼å¼å›žè¦†ï¼š
{{
  "rankings": [
    {{"model": "A", "score": 8, "reason": "..."}},
    ...
  ],
  "best": "A",
  "summary": "æ•´é«”è©•èªž"
}}"""

# ============================================
# ä¸»æµç¨‹
# ============================================

def load_filtered_news(date: str) -> list:
    """è¼‰å…¥ç¯©é¸å¾Œçš„æ–°èž"""
    path = Path(f"data/filtered_{date}.json")
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {path}ï¼Œè«‹å…ˆåŸ·è¡Œ pipeline")
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def sample_news(news_list: list, n: int = 3) -> list:
    """æŠ½æ¨£æ–°èž"""
    # å„ªå…ˆé¸æ“‡é«˜åˆ†æ–°èž
    sorted_news = sorted(news_list, key=lambda x: x.get('score', 0), reverse=True)
    return sorted_news[:n]

def run_experiment(date: str, sample_size: int = 3):
    """åŸ·è¡Œå¯¦é©—"""
    print(f"ðŸ“Š æ¨¡åž‹å“è³ªæ¯”è¼ƒå¯¦é©— - {date}")
    print("=" * 50)

    # 1. è¼‰å…¥ä¸¦æŠ½æ¨£æ–°èž
    all_news = load_filtered_news(date)
    samples = sample_news(all_news, sample_size)
    print(f"ðŸ“° æŠ½æ¨£ {len(samples)} å‰‡æ–°èžé€²è¡Œæ¸¬è©¦\n")

    results = []

    for i, news in enumerate(samples, 1):
        print(f"\n--- æ–°èž {i}/{len(samples)}: {news.get('title', 'N/A')[:50]}... ---\n")

        # 2. æ¯å€‹ CP æ¨¡åž‹ç”Ÿæˆæ‘˜è¦
        outputs = {}
        for name, config in CP_MODELS.items():
            print(f"  ðŸ”„ {name}...", end=" ", flush=True)
            prompt = TASK_PROMPT.format(
                title=news.get('title', ''),
                source=news.get('source', ''),
                summary=news.get('summary', ''),
                link=news.get('link', '')
            )
            output = call_model(config, "ä½ æ˜¯å°ˆæ¥­çš„ AI ç§‘æŠ€æ–°èžç·¨è¼¯ã€‚", prompt)
            outputs[name] = output
            print("âœ…" if not output.startswith("[ERROR]") else "âŒ")

        # 3. é ‚é…æ¨¡åž‹è©•å¯©ï¼ˆç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„ï¼‰
        outputs_text = "\n\n".join([
            f"ã€æ¨¡åž‹ {chr(65+i)}ã€‘\n{text}"
            for i, (name, text) in enumerate(outputs.items())
        ])
        model_map = {chr(65+i): name for i, name in enumerate(outputs.keys())}

        judge_prompt = JUDGE_PROMPT.format(
            title=news.get('title', ''),
            summary=news.get('summary', ''),
            outputs=outputs_text
        )

        print(f"\n  âš–ï¸ è©•å¯©ä¸­ (gemini-pro-top)...", end=" ", flush=True)
        try:
            judge_result = call_model(
                JUDGE_MODELS["gemini-pro-top"],
                "ä½ æ˜¯è³‡æ·± AI å…§å®¹å“è³ªè©•å¯©ï¼Œè«‹å®¢è§€å…¬æ­£åœ°è©•ä¼°ã€‚",
                judge_prompt
            )
            print("âœ…")
        except Exception as e:
            print(f"âŒ {e}")
            judge_result = "{}"

        results.append({
            "news": news.get('title', ''),
            "outputs": outputs,
            "model_map": model_map,
            "judge_result": judge_result
        })

    # 4. ä¿å­˜çµæžœ
    output_dir = Path("experiments")
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f"compare_{date}.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å¯¦é©—å®Œæˆï¼çµæžœå·²ä¿å­˜: {output_path}")
    return results

def print_summary(results: list):
    """è¼¸å‡ºæ‘˜è¦"""
    print("\n" + "=" * 50)
    print("ðŸ“Š å¯¦é©—çµæžœæ‘˜è¦")
    print("=" * 50)

    for r in results:
        print(f"\nðŸ“° {r['news'][:50]}...")
        print(f"   è©•å¯©çµæžœ: {r['judge_result'][:200]}...")

if __name__ == "__main__":
    import sys
    date = sys.argv[1] if len(sys.argv) > 1 else datetime.now().strftime("%Y-%m-%d")
    sample_size = int(sys.argv[2]) if len(sys.argv) > 2 else 3
    results = run_experiment(date, sample_size=sample_size)
    print_summary(results)
