#!/usr/bin/env python3
"""
Thinker News æ¯æ—¥æ–°èè‡ªå‹•ç”Ÿæˆç³»çµ±

æ ¸å¿ƒæµç¨‹ï¼š
1. RSS feeds è®€å–
2. å°ç£æœ¬åœ°åŒ–ç¯©é¸
3. AI è™•ç†éˆï¼ˆDeepSeek â†’ OpenAI â†’ OpenAI â†’ DeepSeekï¼‰
4. HTML é é¢ç”Ÿæˆ
5. è¼¸å‡º latest.json
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_generation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

from rss_fetcher import fetch_all_rss_feeds
from news_filter import filter_and_score_news
from ai_processor import (
    get_openai_client,
    get_deepseek_client,
    process_with_data_alchemist,
    process_with_tech_narrator,
    process_with_editor_in_chief,
    process_with_html_generator
)
from html_generator import generate_daily_html, update_index_html
from utils import get_taiwan_date, validate_json_output
from execution_logger import ExecutionLogger


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def retry_call(fn, args=(), kwargs=None, max_retries=2, step_name=""):
    """å°å‡½å¼åš retryï¼Œæ¯æ¬¡å¤±æ•—ç­‰å¾… 5 ç§’å†é‡è©¦ã€‚

    Returns:
        fn çš„å›å‚³å€¼
    Raises:
        æœ€å¾Œä¸€æ¬¡ä¾‹å¤–ï¼ˆè‹¥å…¨éƒ¨å¤±æ•—ï¼‰
    """
    kwargs = kwargs or {}
    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            last_error = e
            logger.warning(f"âš ï¸ [{step_name}] ç¬¬ {attempt} æ¬¡å¤±æ•—: {e}")
            if attempt < max_retries:
                logger.info(f"â³ [{step_name}] {5}s å¾Œé‡è©¦...")
                time.sleep(5)
    raise last_error


def log_step(exec_logger, name, node_type, description, fn, *args, **kwargs):
    """åŸ·è¡Œä¸€å€‹ pipeline æ­¥é©Ÿä¸¦è‡ªå‹•è¨˜éŒ„ exec_loggerã€‚

    Returns:
        fn çš„å›å‚³å€¼
    Raises:
        fn æ‹‹å‡ºçš„ä¾‹å¤–ï¼ˆå·²è¨˜éŒ„åˆ° exec_loggerï¼‰
    """
    exec_logger.log_node_start(name, node_type, description)
    try:
        result = fn(*args, **kwargs)
        return result
    except Exception as e:
        exec_logger.log_node_error(name, e)
        raise


# ---------------------------------------------------------------------------
# Pipeline Steps
# ---------------------------------------------------------------------------

def step_fetch_rss(today_date):
    """æ­¥é©Ÿ 2: è®€å– RSS feeds"""
    all_feeds = fetch_all_rss_feeds(today_date)
    if not all_feeds:
        raise RuntimeError("RSS è®€å–çµæœç‚ºç©ºï¼Œç„¡æ³•ç¹¼çºŒ")
    logger.info(f"ğŸ“¡ è®€å– {len(all_feeds)} å‰‡æ–°è")
    return all_feeds


def step_filter_news(all_feeds, today_date):
    """æ­¥é©Ÿ 3: ç¯©é¸èˆ‡è©•åˆ†"""
    filtered = filter_and_score_news(all_feeds, today_date)
    if not filtered:
        raise RuntimeError("æ²’æœ‰æ–°èé€šéç¯©é¸ï¼Œæµç¨‹çµ‚æ­¢")
    local = sum(1 for n in filtered if n.get('is_taiwan_news', False))
    logger.info(f"ğŸ” ç¯©é¸å¾Œ {len(filtered)} å‰‡ï¼ˆå°ç£ {local} / åœ‹éš› {len(filtered) - local}ï¼‰")
    return filtered


def step_ai_chain(filtered_news, today_date):
    """æ­¥é©Ÿ 4: AI å››éšæ®µè™•ç†éˆï¼Œæ¯éšæ®µå¸¶ retry"""

    # 4.1 æ•¸æ“šç…‰é‡‘è¡“å¸« (DeepSeek)
    logger.info("  âš—ï¸  æ•¸æ“šç…‰é‡‘è¡“å¸«...")
    raw = retry_call(
        process_with_data_alchemist,
        args=(filtered_news, today_date),
        step_name="æ•¸æ“šç…‰é‡‘è¡“å¸«"
    )
    alchemist_json = validate_json_output(raw, "æ•¸æ“šç…‰é‡‘è¡“å¸«")

    # 4.2 ç§‘æŠ€å°è®€äºº (OpenAI)
    logger.info("  ğŸ“° ç§‘æŠ€å°è®€äºº...")
    raw = retry_call(
        process_with_tech_narrator,
        args=(alchemist_json, today_date),
        step_name="ç§‘æŠ€å°è®€äºº"
    )
    narrator_json = validate_json_output(raw, "ç§‘æŠ€å°è®€äºº")

    # 4.3 ç¸½ç·¨è¼¯ (OpenAI)
    logger.info("  âœï¸  ç¸½ç·¨è¼¯...")
    raw = retry_call(
        process_with_editor_in_chief,
        args=(narrator_json, today_date),
        step_name="ç¸½ç·¨è¼¯"
    )
    editor_json = validate_json_output(raw, "ç¸½ç·¨è¼¯")

    # 4.4 HTML ç”Ÿæˆå™¨ (DeepSeek)
    logger.info("  ğŸ¨ HTML ç”Ÿæˆå™¨...")
    html_content = retry_call(
        process_with_html_generator,
        kwargs={
            "notion_content": narrator_json.get('notion_daily_report_text', ''),
            "line_content": editor_json.get('line_message_text', ''),
            "today_date": today_date,
        },
        step_name="HTML ç”Ÿæˆå™¨"
    )

    return narrator_json, editor_json, html_content


def step_generate_output(today_date, narrator_json, editor_json, html_content):
    """æ­¥é©Ÿ 5-7: çµ„è£è¼¸å‡ºã€å¯« HTMLã€å¯« latest.json"""

    notion_content = narrator_json.get('notion_daily_report_text', '')
    line_content = editor_json.get('line_message_text', '')
    website_url = f"https://thinkercafe-tw.github.io/thinker-news/{today_date}.html"

    final_output = {
        'final_date': today_date,
        'notion_content': notion_content,
        'line_content': line_content,
        'website_url': website_url,
        'news_json': {
            'date': today_date,
            'line_content': line_content,
            'notion_content': notion_content,
            'website_url': website_url,
            'generated_at': datetime.now().isoformat()
        }
    }

    # ç”Ÿæˆ HTML
    daily_path = generate_daily_html(final_output, html_content)
    index_path = update_index_html(today_date)
    logger.info(f"ğŸ“ HTML: {daily_path}, {index_path}")

    # å¯«å…¥ latest.json
    with open('latest.json', 'w', encoding='utf-8') as f:
        json.dump(final_output['news_json'], f, ensure_ascii=False, indent=2)
    logger.info("ğŸ’¾ latest.json å·²å„²å­˜")

    return final_output


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    """ä¸»åŸ·è¡Œæµç¨‹"""
    exec_logger = ExecutionLogger()

    try:
        # æ­¥é©Ÿ 0: é©—è­‰ API Keysï¼ˆå–®ä¾‹åˆå§‹åŒ–ï¼‰
        logger.info("ğŸ”‘ é©—è­‰ API Keys...")
        get_openai_client()
        get_deepseek_client()

        # æ­¥é©Ÿ 1: ä»Šæ—¥æ—¥æœŸ
        today_date = get_taiwan_date()
        logger.info(f"ğŸ“… ä»Šæ—¥æ—¥æœŸ: {today_date}")

        # æ­¥é©Ÿ 2: RSS
        all_feeds = log_step(
            exec_logger, "RSS Feed è®€å–", "rss",
            "è®€å–æ‰€æœ‰æ–°èä¾†æºçš„ RSS feeds",
            step_fetch_rss, today_date
        )
        sources = {}
        for f in all_feeds:
            s = f.get('source', 'unknown')
            sources[s] = sources.get(s, 0) + 1
        exec_logger.log_node_success("RSS Feed è®€å–", {"total": len(all_feeds), "sources": sources},
                                     {"ç¸½æ–°èæ•¸": f"{len(all_feeds)} å‰‡"})

        # æ­¥é©Ÿ 3: ç¯©é¸
        filtered_news = log_step(
            exec_logger, "å°ç£æœ¬åœ°åŒ–ç¯©é¸", "filter",
            "ç¯©é¸å’Œæ’åºæ–°è",
            step_filter_news, all_feeds, today_date
        )
        local_count = sum(1 for n in filtered_news if n.get('is_taiwan_news', False))
        exec_logger.log_node_success("å°ç£æœ¬åœ°åŒ–ç¯©é¸", {"count": len(filtered_news)},
                                     {"ç¯©é¸å¾Œ": f"{len(filtered_news)} å‰‡",
                                      "å°ç£": f"{local_count}", "åœ‹éš›": f"{len(filtered_news) - local_count}"})

        # æ­¥é©Ÿ 4: AI è™•ç†éˆ
        exec_logger.log_node_start("AI è™•ç†éˆ", "ai", "å››éšæ®µ AI è™•ç†")
        narrator_json, editor_json, html_content = step_ai_chain(filtered_news, today_date)
        exec_logger.log_node_success("AI è™•ç†éˆ", None, {"éšæ®µ": "4/4 å®Œæˆ"})
        logger.info("âœ… AI è™•ç†éˆå®Œæˆ")

        # æ­¥é©Ÿ 5-7: è¼¸å‡º
        final_output = log_step(
            exec_logger, "è¼¸å‡ºç”Ÿæˆ", "html",
            "ç”Ÿæˆ HTML + latest.json",
            step_generate_output, today_date, narrator_json, editor_json, html_content
        )
        exec_logger.log_node_success("è¼¸å‡ºç”Ÿæˆ", None, {"æª”æ¡ˆ": "3 å€‹"})

        # å®Œæˆ
        logger.info("ğŸ‰ æ–°èç”Ÿæˆæµç¨‹å®Œæˆï¼")
        logger.info(f"ğŸ“Š åŸå§‹ {len(all_feeds)} â†’ ç¯©é¸ {len(filtered_news)} â†’ {today_date}.html")

        exec_logger.complete_execution("success")
        exec_logger.save_to_file("execution_log.json")
        return 0

    except Exception as e:
        logger.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}", exc_info=True)
        try:
            exec_logger.complete_execution("error")
            exec_logger.save_to_file("execution_log.json")
        except Exception:
            pass
        return 1


if __name__ == "__main__":
    sys.exit(main())
